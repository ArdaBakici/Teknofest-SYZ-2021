{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Here is the imports\n",
    "import os\n",
    "from tensorflow.python.keras import optimizers\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "FLAGS = [] # tensorboard, mixed_precision\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "if \"mixed_precision\" in FLAGS:\n",
    "    print(f\"PreTrain: Using Mixed Policy float16\")\n",
    "    keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "from matplotlib import pyplot as plt\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "import random\n",
    "import albumentations as A\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "import efficientnet.tfkeras as eff"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DATASET_PATH = \"./recordbase\"\n",
    "TRAIN_DIR = \"train\"\n",
    "VAL_DIR = \"val\"\n",
    "TEST_DIR = \"test\"\n",
    "\n",
    "RECORD_ENCODING_TYPE = \"ZLIB\" # none if no encoding is used\n",
    "\n",
    "# Pipeline parameters\n",
    "BUFFER_SIZE = None # set buffer size to default value, change if you have bottleneck\n",
    "SHUFFLE_SIZE = 256 # because dataset is too large huge shuffle sizes may cause problems with ram\n",
    "BATCH_SIZE = 2 # Highly dependent on d-gpu and system ram\n",
    "STEPS_PER_EPOCH = 4977//BATCH_SIZE # 4646 IMPORTANT this value should be equal to file_amount/batch_size because we can't find file_amount from tf.Dataset you should note it yourself\n",
    "VAL_STEPS_PER_EPOCH = 1659//BATCH_SIZE # 995 same as steps per epoch\n",
    "MODEL_WEIGHTS_PATH = None#'./models/14_09-22_55/best.h5' # if not none model will be contiune training with these weights\n",
    "# every shard is 200 files with 36 files on last shard\n",
    "# Model Constants\n",
    "BACKBONE = 'efficientnetb3'\n",
    "# inme_yok, inme_var\n",
    "CLASSES = ['inme_yok', 'inme_var']\n",
    "LR = 0.0001\n",
    "IMG_SIZE = 512\n",
    "FIRST_EPOCHS = 10\n",
    "FINE_TUNE_EPOCHS = 10\n",
    "MODEL_SAVE_PATH = \"./models\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "specifier_name = 'phase1_efficent_net'\n",
    "date_name = f'{datetime.now().strftime(\"%d_%m-%H_%M\")}-{specifier_name}'\n",
    "\n",
    "# Variables\n",
    "train_dir = os.path.join(DATASET_PATH, TRAIN_DIR)\n",
    "val_dir = os.path.join(DATASET_PATH, VAL_DIR)\n",
    "\n",
    "train_filenames = tf.io.gfile.glob(f\"{train_dir}/*.tfrecords\")\n",
    "val_filenames = tf.io.gfile.glob(f\"{val_dir}/*.tfrecords\")\n",
    "\n",
    "random.shuffle(train_filenames) # shuffle tfrecord files order\n",
    "random.shuffle(val_filenames)\n",
    "\n",
    "os.makedirs(f'{MODEL_SAVE_PATH}/{date_name}', exist_ok=True)\n",
    "\n",
    "# define callbacks for learning rate scheduling and best checkpoints saving\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(f'{MODEL_SAVE_PATH}/{date_name}/best.h5', save_weights_only=False, save_best_only=True, mode='min'),\n",
    "    keras.callbacks.ModelCheckpoint(f'{MODEL_SAVE_PATH}/{date_name}/epoch_{{epoch:02d}}.h5', save_weights_only=False, save_freq=STEPS_PER_EPOCH*10, save_best_only=False, mode='min'),\n",
    "    #keras.callbacks.ModelCheckpoint(f'{MODEL_SAVE_PATH}/{date_name}/weights_{{epoch:02d}}.h5', save_weights_only=True, save_freq=STEPS_PER_EPOCH*5, save_best_only=False, mode='min'),\n",
    "    keras.callbacks.ReduceLROnPlateau(),\n",
    "    keras.callbacks.CSVLogger(f'./customlogs/{date_name}.csv')\n",
    "]\n",
    "\n",
    "if \"tensorboard\" in FLAGS:\n",
    "    print(f\"PreTrain: Using tensorboard\")\n",
    "    callbacks.append(\n",
    "        TensorBoard(\n",
    "            log_dir=\"logs\",\n",
    "            histogram_freq=0,\n",
    "            write_graph=True,\n",
    "            write_images=False,\n",
    "            update_freq=\"epoch\",\n",
    "        ))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def visualize(**images):\n",
    "    \"\"\"Plot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        # if whole binary image is true plt shows it as whole image is false so for bypassing this issue we assing one pixels value to 0\n",
    "        image[1,1] = 0 \n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "def aug_fn(image):\n",
    "    transforms = A.Compose([\n",
    "            A.Rotate(limit=40),\n",
    "            A.Flip(),\n",
    "            ])\n",
    "    aug_data = transforms(image=image)\n",
    "    aug_img = aug_data[\"image\"]\n",
    "    return aug_img\n",
    "\n",
    "def preprocessing_fn(image, mask):\n",
    "    image = image.astype(\"float32\")\n",
    "    return image, mask \n",
    "\n",
    "def parse_examples_batch(examples):\n",
    "    feature_description = {\n",
    "        'image' : tf.io.FixedLenFeature([], tf.string),\n",
    "        'label' : tf.io.FixedLenFeature([], tf.float32)\n",
    "    }\n",
    "    samples = tf.io.parse_example(examples, feature_description)\n",
    "    return samples\n",
    "\n",
    "def prepare_sample(features):\n",
    "    image = np.array(0)\n",
    "    #image = tf.vectorized_map(lambda x: tf.io.parse_tensor(x, out_type = tf.uint8), features[\"image\"])\n",
    "    label = features[\"label\"]\n",
    "    #label = np.array(0)\n",
    "    #image, label = tf.vectorized_map(lambda x: tf.numpy_function(func=preprocessing_fn, inp=x, Tout=(tf.float32, tf.float32)), [image, label])\n",
    "    #image = tf.vectorized_map(lambda x: tf.reshape(x, [512, 512, 3]), [image])\n",
    "    return image, label\n",
    "\n",
    "def prepare_sample_aug(features):\n",
    "    image = tf.vectorized_map(lambda x: tf.io.parse_tensor(x, out_type = tf.uint8), features[\"image\"])\n",
    "    label = features[\"label\"]\n",
    "    image = tf.vectorized_map(lambda x: tf.numpy_function(func=aug_fn, inp=x, Tout=(tf.uint8)), [image])\n",
    "    image, label = tf.vectorized_map(lambda x: tf.numpy_function(func=preprocessing_fn, inp=x, Tout=(tf.float32, tf.float32)), [image, label])\n",
    "    return image, label\n",
    "\n",
    "def get_dataset_optimized(filenames, batch_size, epoch_num, shuffle_size, augment=True):\n",
    "    record_dataset = tf.data.TFRecordDataset(filenames, compression_type=RECORD_ENCODING_TYPE, num_parallel_reads=AUTOTUNE)\n",
    "    if shuffle_size > 0:\n",
    "        record_dataset = record_dataset.shuffle(shuffle_size)\n",
    "    record_dataset = (record_dataset\n",
    "                    .repeat(epoch_num)\n",
    "                    .batch(batch_size=batch_size)\n",
    "                    .map(map_func=parse_examples_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE))\n",
    "    if augment:\n",
    "        record_dataset = record_dataset.map(map_func=prepare_sample_aug, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    else:\n",
    "        record_dataset = record_dataset.map(map_func=prepare_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    return record_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sanity_dataset = get_dataset_optimized(train_filenames, BATCH_SIZE, FIRST_EPOCHS, 0, False)\n",
    "\n",
    "i = sanity_dataset.take(15)\n",
    "for x in i:\n",
    "    for k in range(0, 2):\n",
    "        img, label = x\n",
    "        image = np.expand_dims(img[k].numpy(), axis=0)\n",
    "        visualize(imaj=image.squeeze())\n",
    "        print(label[k])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "model = EfficientNetB4(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=inputs,\n",
    "    input_shape=(512, 512, 3),\n",
    ")\n",
    "\n",
    "#model = eff.EfficientNetB4(include_top=False,\n",
    "#        weights='noisy-student',\n",
    "#        input_tensor=x,\n",
    "#        input_shape=(512, 512, 3))\n",
    "\n",
    "model.trainable = False\n",
    "\n",
    "# Rebuild top\n",
    "x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "top_dropout_rate = 0.2\n",
    "x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\", name=\"pred\")(x)\n",
    "\n",
    "model = keras.models.Model(inputs, outputs, name=\"EfficientNet\")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics=[\"accuracy\"]\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "history = model.fit(\n",
    "        get_dataset_optimized(train_filenames, BATCH_SIZE, FIRST_EPOCHS, SHUFFLE_SIZE, augment=False), \n",
    "        steps_per_epoch=STEPS_PER_EPOCH, \n",
    "        epochs=FIRST_EPOCHS, \n",
    "        callbacks=callbacks, \n",
    "        validation_data=get_dataset_optimized(val_filenames, BATCH_SIZE, FIRST_EPOCHS, 0, augment=False), \n",
    "        validation_steps=VAL_STEPS_PER_EPOCH,\n",
    "        #initial_epoch=5\n",
    "    )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_177371/4269439729.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mget_dataset_optimized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_filenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFIRST_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSHUFFLE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFIRST_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Coding/teknofest-venv/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for layer in model.layers[-20:]:\n",
    "    if not isinstance(layer, layers.BatchNormalization):\n",
    "        layer.trainable = True\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "    model.compile(\n",
    "    optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics=[\"accuracy\"]\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "history = model.fit(\n",
    "        get_dataset_optimized(train_filenames, BATCH_SIZE, FIRST_EPOCHS, SHUFFLE_SIZE, augment=True), \n",
    "        steps_per_epoch=STEPS_PER_EPOCH, \n",
    "        epochs=FINE_TUNE_EPOCHS, \n",
    "        callbacks=callbacks, \n",
    "        validation_data=get_dataset_optimized(val_filenames, BATCH_SIZE, FIRST_EPOCHS, 0, augment=False), \n",
    "        validation_steps=VAL_STEPS_PER_EPOCH,\n",
    "        initial_epoch=FIRST_EPOCHS\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('teknofest-venv': venv)"
  },
  "interpreter": {
   "hash": "c3b109d40fe907ead7ade9c1b48b16b3bef4465b369e345f2adfa6bf3aa7e6e2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}