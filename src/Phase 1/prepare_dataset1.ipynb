{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\n",
    "import os \n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import trange\n",
    "import random\n",
    "import albumentations as A"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Dataset Constants\n",
    "DATASET_SPLIT = {\"train\": 6486, \"val\" : 150} # split_name: [amount_of_files, preprocessing_func] | you can leave amount of files None if you don't wan't to split\n",
    "NORMAL_SPLIT = {\"train\": 3310, \"val\" : 1103} # split_name: [amount_of_files, preprocessing_func] | you can leave amount of files None if you don't wan't to split\n",
    "ANORMAL_SPLIT = {\"train\": 1668 , \"val\" : 555}\n",
    "\n",
    "DATASET_PATH = \"./data\"\n",
    "NORMAL_DIR = \"inme_yok\"\n",
    "ANORMAL_DIR = \"inme_var\"\n",
    "\n",
    "CLASS_NAMES = ['inme_yok', 'inme_var']\n",
    "CLASSES = [0, 1]\n",
    "BACKBONE = 'efficientnetb3' # enter the preprocessing for model to here, leave none if don't want to the preprocessing\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "LABEL_DIR = \"label\"\n",
    "\n",
    "IMG_EXT = \"png\"\n",
    "\n",
    "OUT_PATH = \"./outdata/tfrecord/\"\n",
    "\n",
    "ENCODING_TYPE = \"ZLIB\" # zlib, gzip or none - encoding  increases preprocessing time but reduces size by HUGE AMOUNTS (about %96 percent) \n",
    "\n",
    "MAX_FILES = 600"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def image_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(value=[serialize_array(value)])\n",
    "    )\n",
    "\n",
    "def bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def int64_feature_list(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def float_feature_list(value):\n",
    "    \"\"\"Returns a list of float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "# non keras\n",
    "def serialize_array(array):\n",
    "  array = tf.io.serialize_tensor(array).numpy()\n",
    "  return array\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        A.Lambda(image=preprocessing_fn),\n",
    "    ]\n",
    "    return A.Compose(_transform)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def parse_single_image(image, label):\n",
    "  \n",
    "  #define the dictionary -- the structure -- of our single example\n",
    "  data = {\n",
    "        'image' : image_feature(image),\n",
    "        'label' : float_feature(label)\n",
    "    }\n",
    "  #create an Example, wrapping the single features\n",
    "  out = tf.train.Example(features=tf.train.Features(feature=data))\n",
    "  return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def get_file_paths(img_normal_path, img_anormal_path, randomize=True):\n",
    "    img_filenames_w_labels = []\n",
    "    for i in tf.io.gfile.glob(f\"{img_normal_path}/*.{IMG_EXT}\"):\n",
    "        img_filenames_w_labels.append([i, 0])\n",
    "    for i in tf.io.gfile.glob(f\"{img_anormal_path}/*.{IMG_EXT}\"):\n",
    "        img_filenames_w_labels.append([i, 1])\n",
    "\n",
    "    if randomize:\n",
    "        random.shuffle(img_filenames_w_labels)\n",
    "\n",
    "    return img_filenames_w_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main Processing Function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def write_image_batches_to_tfr(img_w_labels, filename:str=\"batch\", max_files:int=100, out_dir:str=\"/data/tfrecord/\", augmentation=None, preprocessing=None):\n",
    "    num_of_files = []\n",
    "    for i in range(len(CLASSES)):\n",
    "        num_of_files.append(0)\n",
    "    # determine the number of shards (single TFRecord files) we need:\n",
    "    splits = (len(img_w_labels)//max_files) + 1\n",
    "    if len(img_w_labels)%max_files == 0:\n",
    "        splits-=1\n",
    "    print(f\"\\nUsing {splits} shard(s) for {len(img_w_labels)} files, with up to {max_files} samples per shard\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    file_count = 0\n",
    "    for i in trange(splits):\n",
    "        current_shard_name = f\"{out_dir}tfrecord_{i+1}in{splits}_{filename}.tfrecords\"\n",
    "        if ENCODING_TYPE is not None:\n",
    "            options = tf.io.TFRecordOptions(compression_type=ENCODING_TYPE)\n",
    "            writer = tf.io.TFRecordWriter(current_shard_name, options=options)\n",
    "        else:\n",
    "            writer = tf.io.TFRecordWriter(current_shard_name)\n",
    "\n",
    "        current_shard_count = 0\n",
    "        while current_shard_count < max_files: #as long as our shard is not full\n",
    "            #get the index of the file that we want to parse now\n",
    "            index = i*max_files+current_shard_count\n",
    "            if index == len(img_w_labels): #when we have consumed the whole data, preempt generation\n",
    "                break\n",
    "            \n",
    "            #img = None\n",
    "            #with open(img_filenames[index], 'rb') as file_reader:\n",
    "            #    img = file_reader.read()\n",
    "            img = cv2.imread(img_w_labels[index][0])\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            label = img_w_labels[index][1]\n",
    "            \n",
    "            for counter, value in enumerate(CLASSES):\n",
    "                if label == value:\n",
    "                    num_of_files[counter] += 1\n",
    "            \n",
    "            #create the required Example representation\n",
    "            out = parse_single_image(image=img, label=label)\n",
    "            \n",
    "            writer.write(out.SerializeToString())\n",
    "            current_shard_count+=1\n",
    "            file_count += 1\n",
    "        writer.close()\n",
    "    print(f\"\\nWrote {file_count} elements to TFRecord\")\n",
    "    for count, i in enumerate(num_of_files):\n",
    "        print(f\"{i} files for class {CLASS_NAMES[count]}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(f\"Starting the process.\")\n",
    "normal_img_path = os.path.join(DATASET_PATH, NORMAL_DIR)\n",
    "anormal_img_path = os.path.join(DATASET_PATH, ANORMAL_DIR)\n",
    "\n",
    "img_filenames_w_labels = get_file_paths(normal_img_path, anormal_img_path, randomize=True)\n",
    "last_index = 0\n",
    "print(f\"Info: Total amount of files is {len(img_filenames_w_labels)}\")\n",
    "for split in DATASET_SPLIT:\n",
    "    file_amount = DATASET_SPLIT[split]\n",
    "    print(f\"Info: Starting to process split **{split}** with {file_amount} files\")\n",
    "    split_data = img_filenames_w_labels[last_index:last_index+file_amount]\n",
    "    write_image_batches_to_tfr(split_data, filename=split, max_files=MAX_FILES, out_dir=OUT_PATH, augmentation=None, preprocessing=None)\n",
    "    last_index = last_index+file_amount\n",
    "print(f\"Info: {len(img_filenames_w_labels) - last_index} left over files\")\n",
    "#write_image_batches_to_tfr(split_img, split_label, filename=\"teknofest\", max_files=MAX_FILES, out_dir=OUT_PATH)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting the process.\n",
      "Info: Total amount of files is 6636\n",
      "Info: Starting to process split **train** with 6486 files\n",
      "\n",
      "Using 11 shard(s) for 6486 files, with up to 600 samples per shard\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]2021-09-21 15:32:14.667315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-21 15:32:14.777363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-21 15:32:14.779149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-21 15:32:14.787086: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-21 15:32:14.793297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-21 15:32:14.794955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-21 15:32:14.796348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-21 15:32:16.371415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-21 15:32:16.372211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-21 15:32:16.372615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-21 15:32:16.372981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4690 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2021-09-21 15:32:16.380334: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "100%|██████████| 11/11 [03:37<00:00, 19.80s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Wrote 6486 elements to TFRecord\n",
      "4310 files for class inme_yok\n",
      "2176 files for class inme_var\n",
      "Info: Starting to process split **val** with 150 files\n",
      "\n",
      "Using 1 shard(s) for 150 files, with up to 600 samples per shard\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.20s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Wrote 150 elements to TFRecord\n",
      "103 files for class inme_yok\n",
      "47 files for class inme_var\n",
      "Info: 0 left over files\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('teknofest-venv': venv)"
  },
  "interpreter": {
   "hash": "c3b109d40fe907ead7ade9c1b48b16b3bef4465b369e345f2adfa6bf3aa7e6e2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}