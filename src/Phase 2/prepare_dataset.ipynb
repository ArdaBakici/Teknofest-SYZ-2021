{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\r\n",
    "import os \r\n",
    "import numpy as np\r\n",
    "import cv2\r\n",
    "from tqdm import trange\r\n",
    "import random\r\n",
    "import segmentation_models as sm\r\n",
    "import albumentations as A\r\n",
    "sm.set_framework('tf.keras')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Augmentation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# if you want to augment your data you can put functions on here and pass it down on the dataset_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Dataset Constants\r\n",
    "#DATASET_SPLIT = {\"train\": [4646, None], \"val\" : [995, None], \"test\" : [995, None]} # split_name: [amount_of_files, preprocessing_func] | you can leave amount of files None if you don't wan't to split\r\n",
    "DATASET_SPLIT = {\"train\": 6486, \"val\" : 150} # split_name: [amount_of_files, preprocessing_func] | you can leave amount of files None if you don't wan't to split\r\n",
    "DATASET_PATH = \".\\\\dataset\"\r\n",
    "TRAIN_DIR = \"train\"\r\n",
    "VAL_DIR = \"validation\"\r\n",
    "TEST_DIR = \"test\"\r\n",
    "\r\n",
    "BACKBONE = 'efficientnetb3' # enter the preprocessing for model to here, leave none if don't want to the preprocessing\r\n",
    "\r\n",
    "DATA_DIR = \"data\"\r\n",
    "LABEL_DIR = \"label\"\r\n",
    "\r\n",
    "IMG_EXT = \"png\"\r\n",
    "\r\n",
    "OUT_PATH = \"./outdata/tfrecord/\"\r\n",
    "\r\n",
    "CLASS_VALUES = [1, 2] # unlabelled 0, iskemik 1, hemorajik 2 so output : iskemik 0, hemorajik 1, bg 2\r\n",
    "CLASS_NAMES = [\"Iskemik\", \"Hemorajik\", \"Unlabelled\"]\r\n",
    "\r\n",
    "ENCODING_TYPE = \"ZLIB\" # zlib, gzip or none - encoding  increases preprocessing time but reduces size by HUGE AMOUNTS (about %96 percent) \r\n",
    "\r\n",
    "MAX_FILES = 600"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def image_feature(value):\r\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\r\n",
    "    return tf.train.Feature(\r\n",
    "        bytes_list=tf.train.BytesList(value=[serialize_array(value)])\r\n",
    "    )\r\n",
    "\r\n",
    "def bytes_feature(value):\r\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\r\n",
    "    if isinstance(value, type(tf.constant(0))):\r\n",
    "        value = value.numpy()\r\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n",
    "\r\n",
    "\r\n",
    "def float_feature(value):\r\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\r\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\r\n",
    "\r\n",
    "\r\n",
    "def int64_feature(value):\r\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\r\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n",
    "\r\n",
    "def int64_feature_list(value):\r\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\r\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\r\n",
    "\r\n",
    "def float_feature_list(value):\r\n",
    "    \"\"\"Returns a list of float_list from a float / double.\"\"\"\r\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\r\n",
    "\r\n",
    "# non keras\r\n",
    "def serialize_array(array):\r\n",
    "  array = tf.io.serialize_tensor(array).numpy()\r\n",
    "  return array\r\n",
    "\r\n",
    "def get_preprocessing(preprocessing_fn):\r\n",
    "    \"\"\"Construct preprocessing transform\r\n",
    "    \r\n",
    "    Args:\r\n",
    "        preprocessing_fn (callbale): data normalization function \r\n",
    "            (can be specific for each pretrained neural network)\r\n",
    "    Return:\r\n",
    "        transform: albumentations.Compose\r\n",
    "    \r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    _transform = [\r\n",
    "        A.Lambda(image=preprocessing_fn),\r\n",
    "    ]\r\n",
    "    return A.Compose(_transform)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def parse_single_image(image, label):\r\n",
    "  \r\n",
    "  #define the dictionary -- the structure -- of our single example\r\n",
    "  data = {\r\n",
    "        'image/raw_image' : image_feature(image),\r\n",
    "        'label/raw' : image_feature(label)\r\n",
    "    }\r\n",
    "  #create an Example, wrapping the single features\r\n",
    "  out = tf.train.Example(features=tf.train.Features(feature=data))\r\n",
    "  return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def generate_label_filenames_merge(img_filenames):\r\n",
    "    label_filenames = []\r\n",
    "    for i in img_filenames:\r\n",
    "        current_dir = i.rsplit('\\\\')[-2]\r\n",
    "        label_filenames.append(i.replace(current_dir, f\"{current_dir}annot\"))\r\n",
    "    return label_filenames\r\n",
    "\r\n",
    "def merge_paths(img_path, randomize=True, generate_labels=True):\r\n",
    "    img_filenames = []\r\n",
    "    for i in img_path:\r\n",
    "        img_filenames.extend(tf.io.gfile.glob(f\"{i}/*.{IMG_EXT}\"))\r\n",
    "    if randomize:\r\n",
    "        random.shuffle(img_filenames)\r\n",
    "    if generate_labels:\r\n",
    "        return img_filenames, generate_label_filenames_merge(img_filenames)\r\n",
    "    else:\r\n",
    "        return img_filenames\r\n",
    "\r\n",
    "def generate_label_filenames(img_filenames, img_path, label_path):\r\n",
    "    label_filenames = []\r\n",
    "    for i in img_filenames:\r\n",
    "        label_filenames.append(i.replace(img_path, label_path))\r\n",
    "    return label_filenames\r\n",
    "\r\n",
    "def get_file_paths(img_path, label_path, randomize=True, generate_labels=True):\r\n",
    "    img_filenames = []\r\n",
    "    img_filenames = tf.io.gfile.glob(f\"{img_path}/*.{IMG_EXT}\")\r\n",
    "    if randomize:\r\n",
    "        random.shuffle(img_filenames)\r\n",
    "    if generate_labels:\r\n",
    "        label_filenames = []\r\n",
    "        for i in img_filenames:\r\n",
    "            label_filenames.append(i.replace(img_path, label_path))\r\n",
    "        return img_filenames, label_filenames\r\n",
    "    else:\r\n",
    "        return img_filenames"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main Processing Function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def write_image_batches_to_tfr(img_filenames, label_filenames, filename:str=\"batch\", max_files:int=100, out_dir:str=\"/data/tfrecord/\", augmentation=None, preprocessing=None):\r\n",
    "    double_labelled_num = 0\r\n",
    "    num_of_files = []\r\n",
    "    assert len(img_filenames) == len(label_filenames)\r\n",
    "    for i in range(len(CLASS_VALUES) + 1):\r\n",
    "        num_of_files.append(0)\r\n",
    "    # determine the number of shards (single TFRecord files) we need:\r\n",
    "    splits = (len(img_filenames)//max_files) + 1\r\n",
    "    if len(img_filenames)%max_files == 0:\r\n",
    "        splits-=1\r\n",
    "    print(f\"\\nUsing {splits} shard(s) for {len(img_filenames)} files, with up to {max_files} samples per shard\")\r\n",
    "    os.makedirs(out_dir, exist_ok=True)\r\n",
    "    file_count = 0\r\n",
    "    for i in trange(splits):\r\n",
    "        current_shard_name = f\"{out_dir}tfrecord_{i+1}in{splits}_{filename}.tfrecords\"\r\n",
    "        if ENCODING_TYPE is not None:\r\n",
    "            options = tf.io.TFRecordOptions(compression_type=ENCODING_TYPE)\r\n",
    "            writer = tf.io.TFRecordWriter(current_shard_name, options=options)\r\n",
    "        else:\r\n",
    "            writer = tf.io.TFRecordWriter(current_shard_name)\r\n",
    "\r\n",
    "        current_shard_count = 0\r\n",
    "        while current_shard_count < max_files: #as long as our shard is not full\r\n",
    "            #get the index of the file that we want to parse now\r\n",
    "            index = i*max_files+current_shard_count\r\n",
    "            if index == len(img_filenames): #when we have consumed the whole data, preempt generation\r\n",
    "                break\r\n",
    "            \r\n",
    "            #img = None\r\n",
    "            #with open(img_filenames[index], 'rb') as file_reader:\r\n",
    "            #    img = file_reader.read()\r\n",
    "            img = cv2.imread(img_filenames[index])\r\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n",
    "\r\n",
    "            mask = cv2.imread(label_filenames[index], 0)\r\n",
    "            masks = [(mask == v) for v in CLASS_VALUES]\r\n",
    "            unlabelled = True\r\n",
    "\r\n",
    "            for count, d in enumerate(masks):\r\n",
    "                if np.any(d):\r\n",
    "                    if count == 1:\r\n",
    "                        if not unlabelled:\r\n",
    "                            double_labelled_num += 1\r\n",
    "                    unlabelled = False\r\n",
    "                    num_of_files[count] += 1        \r\n",
    "            \r\n",
    "            if unlabelled:\r\n",
    "                num_of_files[-1] += 1\r\n",
    "\r\n",
    "            mask = np.stack(masks, axis=-1).astype('float32')\r\n",
    "            # add background if mask is not binary\r\n",
    "            if mask.shape[-1] != 1:\r\n",
    "                background = 1 - mask.sum(axis=-1, keepdims=True)\r\n",
    "                mask = np.concatenate((mask, background), axis=-1)\r\n",
    "                \r\n",
    "            #if augmentation is not None:\r\n",
    "            #    sample = augmentation()(image=img, mask=mask)\r\n",
    "            #    img, mask = sample['image'], sample['mask']\r\n",
    "\r\n",
    "            #if preprocessing is not None:\r\n",
    "            #    sample = preprocessing(image=img, mask=mask)\r\n",
    "            #    img, mask = sample['image'], sample['mask']\r\n",
    "\r\n",
    "            #create the required Example representation\r\n",
    "            out = parse_single_image(image=img, label=mask)\r\n",
    "            \r\n",
    "            writer.write(out.SerializeToString())\r\n",
    "            current_shard_count+=1\r\n",
    "            file_count += 1\r\n",
    "        writer.close()\r\n",
    "    print(f\"\\nWrote {file_count} elements to TFRecord\")\r\n",
    "    for count, i in enumerate(num_of_files):\r\n",
    "        print(f\"{i} files for class {CLASS_NAMES[count]}\")\r\n",
    "    print(f\"{double_labelled_num} files for class double\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "print(f\"Starting the process.\")\r\n",
    "img_path = os.path.join(DATASET_PATH, DATA_DIR)\r\n",
    "label_path = os.path.join(DATASET_PATH, LABEL_DIR)\r\n",
    "\r\n",
    "img_filenames, label_filenames = get_file_paths(img_path, label_path)\r\n",
    "\r\n",
    "last_index = 0\r\n",
    "print(f\"Info: Total amount of files is {len(img_filenames)}\")\r\n",
    "for split in DATASET_SPLIT:\r\n",
    "    file_amount = DATASET_SPLIT[split]\r\n",
    "    print(f\"Info: Starting to process split **{split}** with {file_amount} files\")\r\n",
    "    split_img_files = img_filenames[last_index:last_index+file_amount]\r\n",
    "    split_label_files = label_filenames[last_index:last_index+file_amount]\r\n",
    "    write_image_batches_to_tfr(split_img_files, split_label_files, filename=split, max_files=MAX_FILES, out_dir=OUT_PATH, augmentation=None, preprocessing=None)\r\n",
    "    last_index = last_index+file_amount\r\n",
    "print(f\"Info: {len(img_filenames) - last_index} left over files\")\r\n",
    "#write_image_batches_to_tfr(split_img, split_label, filename=\"teknofest\", max_files=MAX_FILES, out_dir=OUT_PATH)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting the process.\n",
      "Info: Total amount of files is 6636\n",
      "Info: Starting to process split **train** with 6486 files\n",
      "\n",
      "Using 11 shard(s) for 6486 files, with up to 600 samples per shard\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [10:17<00:00, 56.09s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Wrote 6486 elements to TFRecord\n",
      "1066 files for class Iskemik\n",
      "1103 files for class Hemorajik\n",
      "4317 files for class Unlabelled\n",
      "0 files for class double\n",
      "Info: Starting to process split **val** with 150 files\n",
      "\n",
      "Using 1 shard(s) for 150 files, with up to 600 samples per shard\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:12<00:00, 12.84s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Wrote 150 elements to TFRecord\n",
      "27 files for class Iskemik\n",
      "27 files for class Hemorajik\n",
      "96 files for class Unlabelled\n",
      "0 files for class double\n",
      "Info: 0 left over files\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make tf record files with train, val and test splitting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "split_imgs = []\r\n",
    "last_index = 0\r\n",
    "for split in DATASET_SPLIT:\r\n",
    "    split_imgs.append(os.path.join(DATASET_PATH, split))\r\n",
    "split_imgs = merge_paths(split_imgs, randomize=False, generate_labels=False)\r\n",
    "print(split_imgs[1861])\r\n",
    "random.shuffle()\r\n",
    "print(f\"Info: Total amount of files is {len(split_imgs)}\")\r\n",
    "for split in DATASET_SPLIT:\r\n",
    "    split_data = DATASET_SPLIT[split]\r\n",
    "    file_amount = split_data[0]\r\n",
    "    print(f\"Info: Starting to process split **{split}** with {file_amount} files\")\r\n",
    "    split_img_files = split_imgs[last_index:last_index+file_amount]\r\n",
    "\r\n",
    "    split_label_files = split_labels[last_index:last_index+file_amount]\r\n",
    "    write_image_batches_to_tfr(split_img_files, split_label_files, filename=split, max_files=MAX_FILES, out_dir=OUT_PATH, augmentation=split_data[1](), preprocessing=get_preprocessing(sm.get_preprocessing(BACKBONE)))\r\n",
    "print(f\"Info: {len(split_imgs) - last_index} left over files\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".\\data\\dataset1\\train\\12459.png\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "shuffle() missing 1 required positional argument: 'x'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-714280de312c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msplit_imgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandomize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerate_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_imgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1861\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Info: Total amount of files is {len(split_imgs)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mDATASET_SPLIT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: shuffle() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "split_imgs = []\r\n",
    "for split in DATASET_SPLIT:\r\n",
    "    split_imgs.append(os.path.join(DATASET_PATH, split))\r\n",
    "split_imgs, split_labels = merge_paths(split_imgs)\r\n",
    "write_image_batches_to_tfr(split_imgs, split_labels, filename='teknofest', max_files=MAX_FILES, out_dir=OUT_PATH, preprocessing=get_preprocessing(sm.get_preprocessing(BACKBONE)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Using 34 shard(s) for 6636 files, with up to 200 samples per shard\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 34/34 [16:57<00:00, 29.92s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Wrote 6636 elements to TFRecord\n",
      "1093 files for class Iskemik\n",
      "1130 files for class Hemorajik\n",
      "4413 files for class Unlabelled\n",
      "0 files for class double\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make tf record files without train, val and test splitting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "print(f\"Starting the process.\")\r\n",
    "img_path = os.path.join(DATASET_PATH, DATA_DIR)\r\n",
    "train_img_path = os.path.join(DATASET_PATH, 'train')\r\n",
    "train_label_path = os.path.join(DATASET_PATH, 'trainannot')\r\n",
    "label_path = os.path.join(DATASET_PATH, LABEL_DIR)\r\n",
    "\r\n",
    "img_filenames = tf.io.gfile.glob(f\"{train_img_path}/*.{IMG_EXT}\") # first get train exceptions\r\n",
    "label_filenames = generate_label_filenames(img_filenames, train_img_path, train_label_path)\r\n",
    "mixed_img, mixed_label = get_file_paths(img_path, label_path)\r\n",
    "img_filenames.extend(mixed_img)\r\n",
    "label_filenames.extend(mixed_label)\r\n",
    "\r\n",
    "last_index = 0\r\n",
    "print(f\"Info: Total amount of files is {len(img_filenames)}\")\r\n",
    "for split in DATASET_SPLIT:\r\n",
    "    split_data = DATASET_SPLIT[split]\r\n",
    "    file_amount = split_data[0]\r\n",
    "    print(f\"Info: Starting to process split **{split}** with {file_amount} files\")\r\n",
    "    split_img_files = img_filenames[last_index:last_index+file_amount]\r\n",
    "    split_label_files = label_filenames[last_index:last_index+file_amount]\r\n",
    "    write_image_batches_to_tfr(split_img_files, split_label_files, filename=split, max_files=MAX_FILES, out_dir=OUT_PATH, augmentation=split_data[1], preprocessing=get_preprocessing(sm.get_preprocessing(BACKBONE)))\r\n",
    "    last_index = last_index+file_amount\r\n",
    "print(f\"Info: {len(split_imgs) - last_index} left over files\")\r\n",
    "#write_image_batches_to_tfr(split_img, split_label, filename=\"teknofest\", max_files=MAX_FILES, out_dir=OUT_PATH)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting the process.\n",
      "Info: Total amount of files is 8497\n",
      "Info: Starting to process split **train** with 4646 files\n",
      "\n",
      "Using 24 shard(s) for 4646 files, with up to 200 samples per shard\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 12%|█▎        | 3/24 [01:44<12:14, 34.97s/it]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-0ee0521af6a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0msplit_img_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_filenames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlast_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlast_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile_amount\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0msplit_label_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_filenames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlast_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlast_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile_amount\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mwrite_image_batches_to_tfr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_img_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_label_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_files\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_FILES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mOUT_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugmentation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBACKBONE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mlast_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlast_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile_amount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Info: {len(split_imgs) - last_index} left over files\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-8baeb7904ac9>\u001b[0m in \u001b[0;36mwrite_image_batches_to_tfr\u001b[1;34m(img_filenames, label_filenames, filename, max_files, out_dir, augmentation, preprocessing)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_single_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m             \u001b[0mcurrent_shard_count\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mfile_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ardab\\Desktop\\Coding\\Teknofest\\venv\\lib\\site-packages\\tensorflow\\python\\lib\\io\\tf_record.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m    311\u001b[0m       \u001b[0mrecord\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \"\"\"\n\u001b[1;32m--> 313\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTFRecordWriter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "92e73c2a3a66178c34ef4af89de09cfd119ed7d275422bdf7065254acf9e400f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}