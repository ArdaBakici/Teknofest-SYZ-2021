{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Here is the imports\r\n",
    "import os\r\n",
    "from tensorflow.python.keras import optimizers\r\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\r\n",
    "FLAGS = [] # tensorboard, mixed_precision\r\n",
    "from tensorflow import keras\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "if \"mixed_precision\" in FLAGS:\r\n",
    "    print(f\"PreTrain: Using Mixed Policy float16\")\r\n",
    "    keras.mixed_precision.set_global_policy('mixed_float16')\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "AUTOTUNE = tf.data.AUTOTUNE\r\n",
    "import random\r\n",
    "import albumentations as A\r\n",
    "from datetime import datetime\r\n",
    "from tensorflow.keras.callbacks import TensorBoard\r\n",
    "from tensorflow.keras import layers\r\n",
    "from tensorflow.keras.models import load_model\r\n",
    "from tensorflow.keras import backend as K\r\n",
    "from tensorflow.keras.applications import EfficientNetB4\r\n",
    "import efficientnet.tfkeras as eff"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DATASET_PATH = \"./recordbase\"\r\n",
    "TRAIN_DIR = \"train\"\r\n",
    "VAL_DIR = \"val\"\r\n",
    "TEST_DIR = \"test\"\r\n",
    "\r\n",
    "RECORD_ENCODING_TYPE = \"ZLIB\" # none if no encoding is used\r\n",
    "\r\n",
    "# Pipeline parameters\r\n",
    "BUFFER_SIZE = None # set buffer size to default value, change if you have bottleneck\r\n",
    "SHUFFLE_SIZE = 256 # because dataset is too large huge shuffle sizes may cause problems with ram\r\n",
    "BATCH_SIZE = 2 # Highly dependent on d-gpu and system ram\r\n",
    "STEPS_PER_EPOCH = 4977//BATCH_SIZE # 4646 IMPORTANT this value should be equal to file_amount/batch_size because we can't find file_amount from tf.Dataset you should note it yourself\r\n",
    "VAL_STEPS_PER_EPOCH = 1659//BATCH_SIZE # 995 same as steps per epoch\r\n",
    "MODEL_WEIGHTS_PATH = None#'./models/14_09-22_55/best.h5' # if not none model will be contiune training with these weights\r\n",
    "# every shard is 200 files with 36 files on last shard\r\n",
    "# Model Constants\r\n",
    "BACKBONE = 'efficientnetb3'\r\n",
    "# inme_yok, inme_var\r\n",
    "CLASSES = ['inme_yok', 'inme_var']\r\n",
    "LR = 0.0001\r\n",
    "IMG_SIZE = 512\r\n",
    "FIRST_EPOCHS = 100\r\n",
    "FINE_TUNE_EPOCHS = 100\r\n",
    "MODEL_SAVE_PATH = \"./models\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "specifier_name = 'phase1_efficent_net'\r\n",
    "date_name = f'{datetime.now().strftime(\"%d_%m-%H_%M\")}-{specifier_name}'\r\n",
    "\r\n",
    "# Variables\r\n",
    "train_dir = os.path.join(DATASET_PATH, TRAIN_DIR)\r\n",
    "val_dir = os.path.join(DATASET_PATH, VAL_DIR)\r\n",
    "\r\n",
    "train_filenames = tf.io.gfile.glob(f\"{train_dir}/*.tfrecords\")\r\n",
    "val_filenames = tf.io.gfile.glob(f\"{val_dir}/*.tfrecords\")\r\n",
    "\r\n",
    "random.shuffle(train_filenames) # shuffle tfrecord files order\r\n",
    "random.shuffle(val_filenames)\r\n",
    "\r\n",
    "os.makedirs(f'{MODEL_SAVE_PATH}/{date_name}', exist_ok=True)\r\n",
    "\r\n",
    "# define callbacks for learning rate scheduling and best checkpoints saving\r\n",
    "callbacks = [\r\n",
    "    keras.callbacks.ModelCheckpoint(f'{MODEL_SAVE_PATH}/{date_name}/best.h5', save_weights_only=False, save_best_only=True, mode='min'),\r\n",
    "    keras.callbacks.ModelCheckpoint(f'{MODEL_SAVE_PATH}/{date_name}/epoch_{{epoch:02d}}.h5', save_weights_only=False, save_freq=STEPS_PER_EPOCH*10, save_best_only=False, mode='min'),\r\n",
    "    #keras.callbacks.ModelCheckpoint(f'{MODEL_SAVE_PATH}/{date_name}/weights_{{epoch:02d}}.h5', save_weights_only=True, save_freq=STEPS_PER_EPOCH*5, save_best_only=False, mode='min'),\r\n",
    "    keras.callbacks.ReduceLROnPlateau(),\r\n",
    "    keras.callbacks.CSVLogger(f'./customlogs/{date_name}.csv')\r\n",
    "]\r\n",
    "\r\n",
    "if \"tensorboard\" in FLAGS:\r\n",
    "    print(f\"PreTrain: Using tensorboard\")\r\n",
    "    callbacks.append(\r\n",
    "        TensorBoard(\r\n",
    "            log_dir=\"logs\",\r\n",
    "            histogram_freq=0,\r\n",
    "            write_graph=True,\r\n",
    "            write_images=False,\r\n",
    "            update_freq=\"epoch\",\r\n",
    "        ))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def aug_fn(image):\r\n",
    "    transforms = A.Compose([\r\n",
    "            A.Rotate(limit=40),\r\n",
    "            A.Flip(),\r\n",
    "            ])\r\n",
    "    aug_data = transforms(image=image)\r\n",
    "    aug_img, aug_mask = aug_data[\"image\"]\r\n",
    "    return aug_img, aug_mask\r\n",
    "\r\n",
    "def preprocessing_fn(image, mask):\r\n",
    "    image, mask = image.astype(\"float32\"), mask.astype(\"float32\")\r\n",
    "    return image, mask \r\n",
    "\r\n",
    "def parse_examples_batch(examples):\r\n",
    "    feature_description = {\r\n",
    "        'image' : tf.io.FixedLenFeature([], tf.string),\r\n",
    "        'label' : tf.io.FixedLenFeature([], tf.int64)\r\n",
    "    }\r\n",
    "    samples = tf.io.parse_example(examples, feature_description)\r\n",
    "    return samples\r\n",
    "\r\n",
    "def prepare_sample(features):\r\n",
    "    image = tf.vectorized_map(lambda x: tf.io.parse_tensor(x, out_type = tf.uint8), features[\"image\"])\r\n",
    "    label = tf.vectorized_map(lambda x: tf.io.parse_tensor(x, out_type = tf.int64), features[\"label\"])\r\n",
    "    #image, label = tf.vectorized_map(lambda x: tf.numpy_function(func=preprocessing_fn, inp=x, Tout=(tf.float32, tf.int64)), [image, label])\r\n",
    "    return image, label\r\n",
    "\r\n",
    "def prepare_sample_aug(features):\r\n",
    "    image = tf.vectorized_map(lambda x: tf.io.parse_tensor(x, out_type = tf.uint8), features[\"image\"])\r\n",
    "    label = tf.vectorized_map(lambda x: tf.io.parse_tensor(x, out_type = tf.int64), features[\"label\"]) # this was float64\r\n",
    "    image = tf.vectorized_map(lambda x: tf.numpy_function(func=aug_fn, inp=x, Tout=(tf.uint8)), image)\r\n",
    "    #image, label = tf.vectorized_map(lambda x: tf.numpy_function(func=preprocessing_fn, inp=x, Tout=(tf.float32, tf.float32)), [image, label])\r\n",
    "    return image, label\r\n",
    "\r\n",
    "def get_dataset_optimized(filenames, batch_size, epoch_num, shuffle_size, augment=True):\r\n",
    "    record_dataset = tf.data.TFRecordDataset(filenames, compression_type=RECORD_ENCODING_TYPE, num_parallel_reads=AUTOTUNE)\r\n",
    "    if shuffle_size > 0:\r\n",
    "        record_dataset = record_dataset.shuffle(shuffle_size)\r\n",
    "    record_dataset = (record_dataset\r\n",
    "                    .repeat(epoch_num)\r\n",
    "                    .batch(batch_size=batch_size)\r\n",
    "                    .map(map_func=parse_examples_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE))\r\n",
    "    if augment:\r\n",
    "        record_dataset = record_dataset.map(map_func=prepare_sample_aug, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n",
    "    else:\r\n",
    "        record_dataset = record_dataset.map(map_func=prepare_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n",
    "    return record_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\r\n",
    "\r\n",
    "model = EfficientNetB4(\r\n",
    "    include_top=False,\r\n",
    "    weights=\"imagenet\",\r\n",
    "    input_tensor=inputs,\r\n",
    "    input_shape=(512, 512, 3),\r\n",
    ")\r\n",
    "\r\n",
    "#model = eff.EfficientNetB4(include_top=False,\r\n",
    "#        weights='noisy-student',\r\n",
    "#        input_tensor=x,\r\n",
    "#        input_shape=(512, 512, 3))\r\n",
    "\r\n",
    "model.trainable = False\r\n",
    "\r\n",
    "# Rebuild top\r\n",
    "x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\r\n",
    "x = layers.BatchNormalization()(x)\r\n",
    "\r\n",
    "top_dropout_rate = 0.2\r\n",
    "x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\r\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\", name=\"pred\")(x)\r\n",
    "\r\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\r\n",
    "model.compile(\r\n",
    "    optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics=[\"accuracy\"]\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "history = model.fit(\r\n",
    "        get_dataset_optimized(train_filenames, BATCH_SIZE, FIRST_EPOCHS, SHUFFLE_SIZE, augment=True), \r\n",
    "        steps_per_epoch=STEPS_PER_EPOCH, \r\n",
    "        epochs=FIRST_EPOCHS, \r\n",
    "        callbacks=callbacks, \r\n",
    "        validation_data=get_dataset_optimized(val_filenames, BATCH_SIZE, FIRST_EPOCHS, 0, augment=False), \r\n",
    "        validation_steps=VAL_STEPS_PER_EPOCH,\r\n",
    "        #initial_epoch=5\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for layer in model.layers[-20:]:\r\n",
    "    if not isinstance(layer, layers.BatchNormalization):\r\n",
    "        layer.trainable = True\r\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\r\n",
    "    model.compile(\r\n",
    "    optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics=[\"accuracy\"]\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "history = model.fit(\r\n",
    "        get_dataset_optimized(train_filenames, BATCH_SIZE, FIRST_EPOCHS, SHUFFLE_SIZE, augment=True), \r\n",
    "        steps_per_epoch=STEPS_PER_EPOCH, \r\n",
    "        epochs=FINE_TUNE_EPOCHS, \r\n",
    "        callbacks=callbacks, \r\n",
    "        validation_data=get_dataset_optimized(val_filenames, BATCH_SIZE, FIRST_EPOCHS, 0, augment=False), \r\n",
    "        validation_steps=VAL_STEPS_PER_EPOCH,\r\n",
    "        initial_epoch=FIRST_EPOCHS\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}