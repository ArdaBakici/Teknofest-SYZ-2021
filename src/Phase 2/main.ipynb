{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Here is the imports\r\n",
    "from tensorflow import keras\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pylot as plt\r\n",
    "import tensorflow as tf\r\n",
    "import segmentation_models as sm\r\n",
    "sm.set_framework('tf.keras')\r\n",
    "# segmentation_models could also use `tf.keras` if you do not have Keras installed\r\n",
    "# or you could switch to other framework using `sm.set_framework('tf.keras')`"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>CONSTANTS</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Dataset Constants\r\n",
    "DATASET_PATH = \".\\\\dataset\\\\\"\r\n",
    "TRAIN_DIR = \"train\"\r\n",
    "VAL_DIR = \"validation\"\r\n",
    "TEST_DIR = \"test\"\r\n",
    "\r\n",
    "DATA_DIR = \"data\"\r\n",
    "LABEL_DIR = \"label\"\r\n",
    "\r\n",
    "# Model Constants\r\n",
    "BACKBONE = 'efficientnetb3'\r\n",
    "BATCH_SIZE = 2\r\n",
    "CLASSES = ['kanama', 'iskemik']\r\n",
    "LR = 0.0001\r\n",
    "EPOCHS = 40\r\n",
    "MODEL_SAVE_PATH = \"./models\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Variables\r\n",
    "x_train_dir = os.path.join(DATASET_PATH, TRAIN_DIR, DATA_DIR)\r\n",
    "y_train_dir = os.path.join(DATASET_PATH, TRAIN_DIR, LABEL_DIR)\r\n",
    "\r\n",
    "x_val_dir = os.path.join(DATASET_PATH, VAL_DIR, DATA_DIR)\r\n",
    "y_val_dir = os.path.join(DATASET_PATH, VAL_DIR, LABEL_DIR)\r\n",
    "\r\n",
    "x_test_dir = os.path.join(DATASET_PATH, TEST_DIR, DATA_DIR)\r\n",
    "y_test_dir = os.path.join(DATASET_PATH, TEST_DIR, LABEL_DIR)\r\n",
    "\r\n",
    "# define callbacks for learning rate scheduling and best checkpoints saving\r\n",
    "callbacks = [\r\n",
    "    keras.callbacks.ModelCheckpoint('./best_model.h5', save_weights_only=True, save_best_only=True, mode='min'),\r\n",
    "    keras.callbacks.ReduceLROnPlateau(),\r\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preprocessing() -> tf.data.Dataset:\r\n",
    "    preprocess_input = sm.get_preprocessing(BACKBONE)\r\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def model_factory() -> keras.Model:\r\n",
    "    # define network parameters\r\n",
    "    n_classes = 1 if len(CLASSES) == 1 else (len(CLASSES) + 1)  # case for binary and multiclass segmentation\r\n",
    "    activation = 'sigmoid' if n_classes == 1 else 'softmax'\r\n",
    "\r\n",
    "    #create model\r\n",
    "    model = sm.Unet(BACKBONE, classes=n_classes, activation=activation)\r\n",
    "\r\n",
    "    # define optomizer\r\n",
    "    optim = keras.optimizers.Adam(LR)\r\n",
    "\r\n",
    "    # Segmentation models losses can be combined together by '+' and scaled by integer or float factor\r\n",
    "    # set class weights for dice_loss (car: 1.; pedestrian: 2.; background: 0.5;)\r\n",
    "    # TODO redefine class weights\r\n",
    "    dice_loss = sm.losses.DiceLoss(class_weights=np.array([1, 2, 0.5])) \r\n",
    "    focal_loss = sm.losses.BinaryFocalLoss() if n_classes == 1 else sm.losses.CategoricalFocalLoss()\r\n",
    "    total_loss = dice_loss + (1 * focal_loss)\r\n",
    "\r\n",
    "    # actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\r\n",
    "    # total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \r\n",
    "\r\n",
    "    metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\r\n",
    "\r\n",
    "    # compile keras model with defined optimozer, loss and metrics\r\n",
    "    model.compile(optim, total_loss, metrics)\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_model(data_generators, model, callbacks):\r\n",
    "    train_dataloader = data_generators[0]\r\n",
    "    valid_dataloader = data_generators[1]\r\n",
    "    # train model\r\n",
    "    history = model.fit(\r\n",
    "        train_dataloader, \r\n",
    "        steps_per_epoch=len(train_dataloader), \r\n",
    "        epochs=EPOCHS, \r\n",
    "        callbacks=callbacks, \r\n",
    "        validation_data=valid_dataloader, \r\n",
    "        validation_steps=len(valid_dataloader),\r\n",
    "    )\r\n",
    "    # TODO make this after eval for writing accuracy in the model name\r\n",
    "    save_path = os.path.join(MODEL_SAVE_PATH, f\"{}acc_{}.{}\")\r\n",
    "    model.save(save_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate_model(eval_generator, model):\r\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_history(train_history):\r\n",
    "    # Plot training & validation iou_score values\r\n",
    "    plt.figure(figsize=(30, 5))\r\n",
    "    plt.subplot(121)\r\n",
    "    plt.plot(train_history.history['iou_score'])\r\n",
    "    plt.plot(train_history.history['val_iou_score'])\r\n",
    "    plt.title('Model iou_score')\r\n",
    "    plt.ylabel('iou_score')\r\n",
    "    plt.xlabel('Epoch')\r\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\r\n",
    "\r\n",
    "    # Plot training & validation loss values\r\n",
    "    plt.subplot(122)\r\n",
    "    plt.plot(train_history.history['loss'])\r\n",
    "    plt.plot(train_history.history['val_loss'])\r\n",
    "    plt.title('Model loss')\r\n",
    "    plt.ylabel('Loss')\r\n",
    "    plt.xlabel('Epoch')\r\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TODO Main code comes here\r\n",
    "generators = preprocessing()\r\n",
    "model = model_factory()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}